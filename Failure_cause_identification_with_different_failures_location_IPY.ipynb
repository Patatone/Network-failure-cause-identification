{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Patatone/Network-failure-cause-identification/blob/main/Failure_cause_identification_with_different_failures_location_IPY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f-Dgj8rcVvCZ"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhyperopt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmt\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Booster\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hyperopt\n",
    "import time\n",
    "import sklearn.metrics as mt\n",
    "import pickle\n",
    "\n",
    "from xgboost import Booster\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wtkni_HDVvCg"
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "###### Define a function load_window_dataset() that takes in input window data file, and \n",
    "###### label to be assigned and returns numpy arrays with features and labels (details below)\n",
    "#################################################################################################\n",
    "\n",
    "def load_window_dataset(X, y, filename, label):\n",
    "#Inputs: - X: current matrix of datapoints where we want to APPEND the datapoints retrieved from filename (only features)\n",
    "#        - y: current matrix of datapoints where we want to APPEND the datapoints retrieved from filename (only labels)\n",
    "#        - filename: full name (with path) of the file to be read (it must be a window dataset file created above)\n",
    "#        - label: integer, label to be assigned to the datapoints retrieved from filename; it may differ from labels already included in current y\n",
    "#Outputs: - X: updated X (including features for the new data points retrieved from filename)\n",
    "#         - y: updated y (including labels for the new data points)\n",
    "#This function should append to X and y in input the new datapoints retrieved from filename and return updated X and y\n",
    "#The function should also handle the case when X and y are empty (initialized as None)\n",
    "\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    if X is None:\n",
    "        X = data.to_numpy()\n",
    "        # full() function puts in all X.shape[0] elements the value \"label\"\n",
    "        y = np.full(X.shape[0], label)\n",
    "    else:\n",
    "        X_temp = data.to_numpy()\n",
    "        y_temp = np.full(X_temp.shape[0], label)\n",
    "        X = np.append(X, X_temp, axis = 0) #F: axis=0-->stack X and X_temp vertically (increase no of rows)\n",
    "        y = np.append(y, y_temp)\n",
    "\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcvMwMfcVvCi",
    "outputId": "371a06b4-3fa7-42a0-efd9-c045e832947f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario_1_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (21591, 6)\n",
      "current shape of y: (21591,)\n",
      "Scenario_1_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (43182, 6)\n",
      "current shape of y: (43182,)\n",
      "Scenario_1_monitor_node_1_preamp_lpth_3-2_1_sp1_w10.dat\n",
      "current shape of X: (64773, 6)\n",
      "current shape of y: (64773,)\n",
      "Scenario_2_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (86364, 6)\n",
      "current shape of y: (86364,)\n",
      "Scenario_2_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (107955, 6)\n",
      "current shape of y: (107955,)\n",
      "Scenario_3_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (129546, 6)\n",
      "current shape of y: (129546,)\n",
      "Scenario_4_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (151137, 6)\n",
      "current shape of y: (151137,)\n",
      "Scenario_4_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (172728, 6)\n",
      "current shape of y: (172728,)\n",
      "Scenario_4_monitor_node_1_preamp_lpth_3-2_1_sp1_w10.dat\n",
      "current shape of X: (194319, 6)\n",
      "current shape of y: (194319,)\n",
      "Scenario_5_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (215910, 6)\n",
      "current shape of y: (215910,)\n",
      "Scenario_5_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (237500, 6)\n",
      "current shape of y: (237500,)\n",
      "Scenario_5_monitor_node_1_preamp_lpth_3-2_1_sp1_w10.dat\n",
      "current shape of X: (259090, 6)\n",
      "current shape of y: (259090,)\n",
      "Scenario_6_monitor_node_1_preamp_lpth_3-2_1_100GHz_sp1_w10.dat\n",
      "current shape of X: (259681, 6)\n",
      "current shape of y: (259681,)\n",
      "Scenario_6_monitor_node_1_preamp_lpth_3-2_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (259972, 6)\n",
      "current shape of y: (259972,)\n",
      "Scenario_6_monitor_node_1_preamp_lpth_3-2_1_25GHz_sp1_w10.dat\n",
      "current shape of X: (260263, 6)\n",
      "current shape of y: (260263,)\n",
      "Scenario_7_monitor_node_1_preamp_lpth_2_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (281854, 6)\n",
      "current shape of y: (281854,)\n",
      "Scenario_7_monitor_node_1_preamp_lpth_3-1_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (303445, 6)\n",
      "current shape of y: (303445,)\n",
      "Scenario_7_monitor_node_1_preamp_lpth_3-2_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (325036, 6)\n",
      "current shape of y: (325036,)\n",
      "Scenario_8_monitor_node_1_preamp_lpth_2_1_50GHz_sp1_w10.dat\n",
      "current shape of X: (346627, 6)\n",
      "current shape of y: (346627,)\n",
      "Scenario_8_monitor_node_1_preamp_lpth_3-1_1_50GHz_sp1_w10.dat\n",
      "current shape of X: (368218, 6)\n",
      "current shape of y: (368218,)\n",
      "Scenario_8_monitor_node_1_preamp_lpth_3-2_1_50GHz_sp1_w10.dat\n",
      "current shape of X: (389809, 6)\n",
      "current shape of y: (389809,)\n",
      "[[18.815 18.815  0.141  0.052 18.893 18.752]\n",
      " [18.816 18.816  0.141  0.052 18.893 18.752]\n",
      " [18.824 18.824  0.141  0.048 18.893 18.752]\n",
      " ...\n",
      " [24.105 24.105  0.109  0.033 24.166 24.057]\n",
      " [24.117 24.117  0.099  0.035 24.175 24.076]\n",
      " [24.123 24.123  0.099  0.036 24.175 24.076]]\n",
      "[0 0 0 ... 1 1 1]\n",
      "(389809, 6)\n",
      "(389809,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'StandardScaler' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Normalization - Comment/Uncomment these lines for Results 1.2\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mStandardScaler\u001b[49m()\n\u001b[0;32m     35\u001b[0m X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(X)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'StandardScaler' is not defined"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "###### Use function load_window_dataset() with datasets of for all scenarios  \n",
    "###### using window length = 10 and spacing = 1. Finally, perform features scaling \n",
    "##########################################################################################\n",
    "\n",
    "# you should pass empty X and y to function load_window_dataset\n",
    "X=None \n",
    "y=None\n",
    "length=10\n",
    "spacing=1\n",
    "folderpath='Features'\n",
    "\n",
    "for filename in os.listdir(folderpath):\n",
    "    #F: you should continue iterating over \"Features\" folder looking for the desired files    \n",
    "    if filename.endswith('_sp' + str(spacing) + '_w' + str(length) + '.dat'):\n",
    "        print(filename)\n",
    "        label = 0\n",
    "        if int(filename[9]) > 5:\n",
    "          label = 1\n",
    "        fullname = folderpath + '/' + filename\n",
    "#------------------------------------------------------------\n",
    "        X, y = load_window_dataset(X, y, fullname, label)\n",
    "#------------------------------------------------------------\n",
    "        print('current shape of X: ' +str(X.shape))\n",
    "        print('current shape of y: ' +str(y.shape))\n",
    "\n",
    "#All scenario correlated ONLY TO \"length=10\" and \"spacing=1\"\n",
    "print(X)\n",
    "print(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Normalization - Comment/Uncomment these lines for Results 1.2\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6dGWj1QVvCj"
   },
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "###### Task 4a) Define function train_classifier_XGB() that performs hyperparameter optimization with ######\n",
    "######          5-fold crossvalidation and training (details below) and returns the trained model ##########\n",
    "######          XGB documentation at: https://xgboost.readthedocs.io/en/stable/python/python_intro.html ####\n",
    "######          XGB hyperparameters: https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html ##\n",
    "######          Part of the function (hyperparameters optimization) is already given #######################\n",
    "######          Hints: use fit() from scikit learn interface for XGB to train the classifier ###############\n",
    "############################################################################################################\n",
    "\n",
    "#Last time we did it with nested for loops\n",
    "def train_classifier_XGB(X_train, y_train, resfileXGB): \n",
    "#Inputs: - X_train: training set (features)\n",
    "#        - y_train: training set (ground truth labels)\n",
    "#        - resfileXGB: full name (with path) of the results file where to put output of the training/optimization process\n",
    "#                      resfileXGB should include info on: \n",
    "#                              1) ML algorithm\n",
    "#                              2) problem (detection or identification)\n",
    "#                              3) window length and window spacing\n",
    "#\n",
    "#This function should: \n",
    "#         * Perform XGBoost hyperparameters optimization via crossvalidation\n",
    "#         * Print hyperparameters obtained with crossvalidation in resfileXGB\n",
    "#         * Retrain an XGB model with best hyperparameters using the entire training set (X_train, y_train)\n",
    "#         * Print training results (best accuracy and training duration) in resfileXGB\n",
    "#         * Return the trained XGB model\n",
    "#\n",
    "#Outputs: - updated resfileXGB with:\n",
    "#                              1) best hyperparameters obtained with crossvalidation\n",
    "#                              2) best accuracy obtained during crossvalidation (for the best hyperparameters)\n",
    "#                              3) duration of training over the entire training set (X_train, y_train)\n",
    "#         - xgb: model to be returned by the function\n",
    "\n",
    "\n",
    "\n",
    "    #F: define the search space for your hyperparameters - a space where to search\n",
    "    # These parameters are needed to balance between underfitting and overfitting\n",
    "    # We are testing 3 hyperparameters: eta, max_depth and subsample\n",
    "    space4xgb = { \n",
    "     'eta': hp.choice('eta', [0.1, 0.3, 0.5, 0.7, 0.9, 1]),\n",
    "     #max_depth (maximum depth of the decision trees being trained)\n",
    "     'max_depth': hp.choice('max_depth', np.arange(1, 20, 2)),\n",
    "     'subsample': hp.choice('subsample', [0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    }\n",
    "\n",
    "    # hyperopt is used to perform an efficent search in the space of parameters\n",
    "    def hyperopt_train_test(params):\n",
    "        model = XGBClassifier(use_label_encoder=False, verbosity = 0, **params)\n",
    "        #F: see https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n",
    "        \n",
    "        return cross_val_score(model, X_train, y_train, cv = 5).mean()\n",
    "        #F: cross_val_score is from scikit learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "        #F: will use the default score (for XGB it is accuracy)\n",
    "        #F: this includes also training; cv=5 (5 number of folds) (5-folds crossvalidation)\n",
    "        #F: .mean() is taken as cross_val_score returns an array of scores (one for each fold)\n",
    "        # We have to do the mean because \"cross_val_score\" returns the accuracy of all the folds\n",
    "\n",
    "    #We pass to this function the \"space4xgb\" parameter (in fmin() function)\n",
    "    #F: this function is used below, as a parameter to fmin\n",
    "    def f(params): \n",
    "        # assumes that \"hyperopt_train_test\" gives use the best cross validation accuracy \n",
    "        # given that combination of hyperparameters (params)\n",
    "        acc = hyperopt_train_test(params)\n",
    "        #F: loss is returned as opposite (negative) of accuracy because we will use in \n",
    "        #f_min (that only minimizes), where we want to minimize the loss (i.e., maximize accuracy)\n",
    "        # We need to return these parameters because fmin() requires them\n",
    "        return {'loss': -acc, 'status': STATUS_OK} #F: loss is returned as opposite (negative) of accuracy because we will use in f_min (that only minimizes), where we want to minimize the loss (i.e., maximize accuracy)\n",
    "\n",
    "    trials = Trials()\n",
    "    #best_params stores the index of the best parameters values according to the function\n",
    "    # fmin() returns the indexes based on the minimum value of a passed function \"f\"\n",
    "    # space4xgb is the search space\n",
    "    # algo=tpe.suggest is the used alorithm\n",
    "    # max_evals=5 is the maximum trials\n",
    "    best_params = fmin(f, space4xgb, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "    #F: see: https://github.com/hyperopt/hyperopt/blob/master/hyperopt/fmin.py\n",
    "    #F: at this point, best_param is a dictionary where each key is the index of the corresponding best param in space4xgb\n",
    "    print(best_params)\n",
    "    \n",
    "    #Insert in the paramets the values of the hyperparameters (not the indexes)\n",
    "    best_params = hyperopt.space_eval(space4xgb, best_params)\n",
    "    #F: this is used to extract from space4xgb the best values according to the indexes in best_params (and put such values in best_params)\n",
    "    print(best_params)\n",
    "    \n",
    "    best_cv_acc = -round(trials.best_trial['result']['loss'], 2) #F: best across trials\n",
    "    print('best_cv_acc: ' + str(best_cv_acc))\n",
    "\n",
    "#######################################################################################################\n",
    "###### Task 4a) (cont.) Now store hyperparameters obtained with crossvalidation in file resfileXGB\n",
    "######                  Train the obtained model using the entire training set and store training time in resfileXGB\n",
    "######                  Return the trained model\n",
    "#######################################################################################################\n",
    "\n",
    "    xgb = XGBClassifier(eta = best_params['eta'], max_depth= best_params['max_depth'], \n",
    "                            subsample = best_params['subsample'], use_label_encoder=False, verbosity = 0) \n",
    "\n",
    "    t0 = time.time()\n",
    "    #F: fit() is a function from scikit learn interface for XGB (https://xgboost.readthedocs.io/en/stable/python/python_intro.html#scikit-learn-interface), \n",
    "    # there is also train() that can be used directly with XGB objects (https://xgboost.readthedocs.io/en/stable/python/python_intro.html#training) \n",
    "    xgb.fit(X_train, y_train) \n",
    "    t1 = time.time()\n",
    "\n",
    "    with open(resfileXGB, 'w') as result_file:\n",
    "        result_file.write('Best eta: {}\\n'.format(best_params['eta']))\n",
    "        result_file.write('Best max depth: {}\\n'.format(best_params['max_depth']))\n",
    "        result_file.write('Best subsample: {}\\n'.format(best_params['subsample']))\n",
    "        result_file.write('Crossvalidation accuracy: {}\\n'.format(best_cv_acc))\n",
    "        result_file.write('Training duration for XGB is {} s\\n'.format(round(t1 - t0)))\n",
    "\n",
    "    return xgb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0aJlN6lVvCl"
   },
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "###### Task 4b) Define similar function train_classifier_DNN() that performs hyperparameter optimization ###\n",
    "######          and training for DNN with given hyperparameters space (details below) ######################\n",
    "############################################################################################################\n",
    "\n",
    "def train_classifier_DNN(X_train, y_train, resfileDNN): \n",
    "#Inputs: - X_train: training set (features)\n",
    "#        - y_train: training set (ground truth labels)\n",
    "#        - resfileDNN: full name (with path) of the results file where to put output of the training/optimization process\n",
    "#                      resfileDNN should include info on: \n",
    "#                              1) ML algorithm\n",
    "#                              2) problem (detection or identification)\n",
    "#                              3) window length and window spacing\n",
    "#\n",
    "#This function should: \n",
    "#         * Perform DNN hyperparameters optimization via crossvalidation\n",
    "#         * Print hyperparameters obtained with crossvalidation in resfileDNN\n",
    "#         * Retrain a DNN model with best hyperparameters using the entire training set (X_train, y_train)\n",
    "#         * Print training results (best accuracy and training duration) in resfileDNN\n",
    "#         * Return the trained DNN model\n",
    "#\n",
    "#Outputs: - updated resfileDNN with:\n",
    "#                              1) best hyperparameters obtained with crossvalidation\n",
    "#                              2) best accuracy obtained during crossvalidation (for the best hyperparameters)\n",
    "#                              3) duration of training over the entire training set (X_train, y_train)\n",
    "#         - dnn: model to be returned by the function\n",
    "############# ADD YOUR CODE BELOW #############\n",
    "\n",
    "    #F: define the search space for your hyperparameters\n",
    "    space4dnn = {\n",
    "     'activation': hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "     'neurons': hp.choice('neurons', [10, 50, 100]),\n",
    "     'layers': hp.choice('layers', np.arange(1, 4, 1))\n",
    "    }\n",
    "\n",
    "    def hyperopt_train_test(params):\n",
    "        size = (params['neurons'],) * params['layers']\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=size, activation=params['activation'],\n",
    "                            solver='adam', learning_rate='invscaling', max_iter=1000)\n",
    "        return cross_val_score(dnn, X_train, y_train, cv = 5).mean()\n",
    "\n",
    "    def f(params):\n",
    "        acc = hyperopt_train_test(params)\n",
    "        return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_params = fmin(f, space4dnn, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "    print(best_params) \n",
    "    \n",
    "    best_params = hyperopt.space_eval(space4dnn, best_params)\n",
    "    print(best_params) \n",
    "    \n",
    "    best_cv_acc = -round(trials.best_trial['result']['loss'], 2)\n",
    "    print('best_cv_acc: ' + str(best_cv_acc))\n",
    "    \n",
    "    \n",
    "    size = (best_params['neurons'],) * best_params['layers']\n",
    "    dnn = MLPClassifier(hidden_layer_sizes=size, activation=best_params['activation'],\n",
    "                                solver='adam', learning_rate='invscaling', max_iter=1000)\n",
    "\n",
    "    t0 = time.time()\n",
    "    dnn.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "\n",
    "    with open(resfileDNN, 'w') as result_file:\n",
    "        result_file.write('Best number of layers: {}\\n'.format(best_params['layers']))\n",
    "        result_file.write('Best number of neurons: {}\\n'.format(best_params['neurons']))\n",
    "        result_file.write('Best activation function: {}\\n'.format(best_params['activation']))\n",
    "        result_file.write('Crossvalidation accuracy: {}\\n'.format(best_cv_acc))\n",
    "        result_file.write('Training duration for DNN is {} s\\n'.format(round(t1 - t0)))\n",
    "\n",
    "\n",
    "    return dnn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SHnCDjAsPrmp"
   },
   "outputs": [],
   "source": [
    "############################################################################################################\n",
    "###### Task 4b) Define similar function train_classifier_kNN() that performs hyperparameter optimization ###\n",
    "######          and training for KNN with given hyperparameters space (details below) ######################\n",
    "############################################################################################################\n",
    "\n",
    "def train_classifier_KNN(X_train, y_train, resfileKNN): \n",
    "\n",
    "    #F: define the search space for your hyperparameters\n",
    "    space4knn = {\n",
    "     'leaf_size': hp.choice('leaf_size', np.arange(1, 50, 1)),\n",
    "     'p': hp.choice('p', [1, 2]),\n",
    "     'n_neighbors': hp.choice('n_neighbors', np.arange(1, 30, 1))\n",
    "    }\n",
    "\n",
    "    def hyperopt_train_test(params):\n",
    "        knn = KNeighborsClassifier(leaf_size=params['leaf_size'], p=params['p'], \n",
    "                                   n_neighbors=params['n_neighbors'])\n",
    "        return cross_val_score(knn, X_train, y_train, cv = 5).mean()\n",
    "\n",
    "    def f(params):\n",
    "        acc = hyperopt_train_test(params)\n",
    "        return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "    trials = Trials()\n",
    "    best_params = fmin(f, space4knn, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "    print(best_params) \n",
    "    \n",
    "    best_params = hyperopt.space_eval(space4knn, best_params)\n",
    "    print(best_params) \n",
    "    \n",
    "    best_cv_acc = -round(trials.best_trial['result']['loss'], 2)\n",
    "    print('best_cv_acc: ' + str(best_cv_acc))\n",
    "    \n",
    "    knn = KNeighborsClassifier(leaf_size=best_params['leaf_size'], p=best_params['p'], \n",
    "                                   n_neighbors=best_params['n_neighbors'])\n",
    "\n",
    "    t0 = time.time()\n",
    "    knn.fit(X_train, y_train)\n",
    "    t1 = time.time()\n",
    "\n",
    "    with open(resfileKNN, 'w') as result_file:\n",
    "        result_file.write('Best leaf_size: {}\\n'.format(best_params['leaf_size']))\n",
    "        result_file.write('Best number of p: {}\\n'.format(best_params['p']))\n",
    "        result_file.write('Best number of neighbors: {}\\n'.format(best_params['n_neighbors']))\n",
    "        result_file.write('Crossvalidation accuracy: {}\\n'.format(best_cv_acc))\n",
    "        result_file.write('Training duration for kNN is {} s\\n'.format(round(t1 - t0)))\n",
    "\n",
    "\n",
    "    return knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF7AXS_bVvCl",
    "outputId": "f6fff8fd-3a66-4554-a277-782e6000d604",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "###### Split (X,y) obtained in task 3b (they had window length=10 and spacing=1) into train/test \n",
    "###### sets (80%/20%). Then, call functions train_classifier_XGB() and train_classifier_DNN(). \n",
    "###### Save trained models (returned by the three functions) into .json files.\n",
    "################################################################################################################\n",
    "res_folder = 'Results'\n",
    "if not os.path.exists(res_folder):\n",
    "    os.makedirs(res_folder)\n",
    "\n",
    "resfile_XGB=res_folder + '/XGB_sp_' + str(spacing) + 'w_' + str(length) + '_results.txt'\n",
    "resfile_DNN=res_folder + '/DNN_sp_' + str(spacing) + 'w_' + str(length) + '_results.txt'\n",
    "resfile_KNN=res_folder + '/KNN_sp_' + str(spacing) + 'w_' + str(length) + '_results.txt'\n",
    "\n",
    "# Stratify garantees the split all the scenarios among train and test\n",
    "# It's like shuffle and split\n",
    "# random_state it's a seed to get the same output\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(y_test)\n",
    "print(X_test)\n",
    "\n",
    "\n",
    "print('Training XGB...')\n",
    "xgb = train_classifier_XGB(X_train, y_train, resfile_XGB)\n",
    "\n",
    "print('Training DNN...')\n",
    "dnn = train_classifier_DNN(X_train, y_train, resfile_DNN)\n",
    "\n",
    "print('Training KNN...')\n",
    "knn = train_classifier_KNN(X_train, y_train, resfile_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dMwKhpgJVLOQ"
   },
   "outputs": [],
   "source": [
    "# Define model names\n",
    "xgbmodelfile = res_folder + '/XGB_sp{}_w{}.json'.format(spacing, length) \n",
    "dnnmodelfile = res_folder + '/DNN_sp{}_w{}.json'.format(spacing, length)\n",
    "knnmodelfile = res_folder + '/KNN_sp{}_w{}.json'.format(spacing, length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EZY6kR17U-8p"
   },
   "outputs": [],
   "source": [
    "# Save the models to disk\n",
    "xgb.save_model(xgbmodelfile)\n",
    "pickle.dump(dnn, open(dnnmodelfile, 'wb'))\n",
    "pickle.dump(knn, open(knnmodelfile, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dosJAeJhVvCm"
   },
   "outputs": [],
   "source": [
    "################################################################################################################\n",
    "###### Task 5a) Define function performance_eval() that takes in input ground truth and predicted labels, ######\n",
    "######          prints results in a result file passed in input, and returns global metrics (details below) ####\n",
    "######          Hints: Use confusion_matrix and other metrics from sklearn.metrics #############################\n",
    "######                 To compute global metrics, use average='weighted' in scikit-learn APIs ##################\n",
    "################################################################################################################\n",
    "def performance_eval(y_true, y_pred, lab, l_names, resfile):\n",
    "#Inputs: - y_true: ground-truth labels\n",
    "#        - y_pred: predicted labels\n",
    "\n",
    "# lab and l_names are two ways of defining the lables\n",
    "# lab = (0,1,2)\n",
    "# l_names = used for plotting = ['Normal', 'Attenuation', 'Filtering']\n",
    "\n",
    "#        - lab: list of labels (integers) as used during training phase\n",
    "#        - l_names: list of labels (categories) corresponding to integer labels \"lab\"; \n",
    "#                   can be customized, it's only used for results plotting\n",
    "#        - resfile: full name (with path) of the results file where to put performance metrics\n",
    "#This function should:            \n",
    "#         * Compute Accuracy, Precision (global-weighted and per class), Recall (global-weighted and per class), F1-score (global-weighted and per class)\n",
    "#         * Write them in result .txt file (resfile passed in input)\n",
    "#         * Compute confusion matrix (cm) and save it in the same result .txt file (resfile)\n",
    "#         * Return Accuracy, overall Precision, overall Recall, overall F1-score\n",
    "#         * Plot of the cm is already given in the code (you need to call the confusion matrix objects as cm and cm_norm)\n",
    "#Outputs: - updated resfileDNN with:\n",
    "#                              1) Accuracy, Precision (global-weighted and per class), Recall (global-weighted and per class), F1-score (global-weighted and per class)\n",
    "#                              2) Confusion matrix (cm) and confusion matrix normalized wrt true labels (cm_norm)\n",
    "#         - return accuracy, global_precision, global_recall, global_f1score (use these variable names!)\n",
    "############# ADD YOUR CODE BELOW #############\n",
    "    \n",
    "    #Compute metrics and print/write them\n",
    "    accuracy = mt.accuracy_score(y_true, y_pred)\n",
    "    precision = mt.precision_score(y_true, y_pred, labels=lab, average=None) #F: average=None gives per-class results\n",
    "    global_precision = mt.precision_score(y_true, y_pred, labels=lab, average='weighted') \n",
    "    recall = mt.recall_score(y_true, y_pred, labels=lab, average=None)\n",
    "    global_recall = mt.recall_score(y_true, y_pred, labels=lab, average='weighted') \n",
    "    f1score = mt.f1_score(y_true, y_pred, labels=lab, average=None)\n",
    "    global_f1score = mt.f1_score(y_true, y_pred, labels=lab, average='weighted')\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] #F: normalized wrt true labels (axis=1 sums the elements of one row and sums them; this is done for all the rows independently)\n",
    "\n",
    "    with open(resfile, 'w') as result_file:\n",
    "        result_file.write('Results for the TEST SET\\n')\n",
    "        result_file.write('Accuracy: {}\\n'.format(accuracy))\n",
    "        result_file.write('Precision per class: {}\\n'.format(precision))\n",
    "        result_file.write('Global Precision: {}\\n'.format(global_precision))\n",
    "        result_file.write('Recall per class: {}\\n'.format(recall))\n",
    "        result_file.write('Global Recall: {}\\n'.format(global_recall))\n",
    "        result_file.write('F1-score: {}\\n'.format(f1score))\n",
    "        result_file.write('Global F1-score: {}\\n'.format(global_f1score))\n",
    "        \n",
    "        result_file.write('\\nConfusion matrix, without normalization\\n')\n",
    "        np.savetxt(result_file, cm, fmt = '%.2f')\n",
    "\n",
    "        result_file.write('\\nNormalized confusion matrix\\n')\n",
    "        #np.savetxt(result_file, cm_norm, fmt = '%.2f')\n",
    "        result_file.write(str(cm_norm))\n",
    "\n",
    "        \n",
    "        \n",
    "########################################################################\n",
    "#F: in the following, we plot the confusion matrices (absolute and normalized one)\n",
    "# ------------------------------- absolute -------------------------------\n",
    "        title = 'Confusion matrix, without normalization'\n",
    "        fig, ax = plt.subplots()\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        # We want to show all ticks...\n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               #xticklabels=lab, yticklabels=lab,\n",
    "               xticklabels=l_names, yticklabels=l_names,\n",
    "               title=title,\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label')\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "        # Loop over data dimensions (#F: i.e., cells in the confusion matrix) and create text annotations. \n",
    "        fmt = 'd'\n",
    "        thresh = cm.max() / 2.\n",
    "        for w in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, w, format(cm[w, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[w, j] > thresh else \"black\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(resfile.replace('.txt','_conf_matrix.png'))\n",
    "        plt.show()\n",
    "# ------------------------------- normalized -------------------------------\n",
    "        title_norm = 'Confusion matrix, with normalization'\n",
    "        fig_n, ax_n = plt.subplots()\n",
    "        im = ax_n.imshow(cm_norm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax_n.figure.colorbar(im, ax=ax_n)\n",
    "        # We want to show all ticks...\n",
    "        ax_n.set(xticks=np.arange(cm_norm.shape[1]),\n",
    "               yticks=np.arange(cm_norm.shape[0]),\n",
    "               # ... and label them with the respective list entries\n",
    "               #xticklabels=lab, yticklabels=lab,\n",
    "               xticklabels=l_names, yticklabels=l_names,\n",
    "               title=title_norm,\n",
    "               ylabel='True label',\n",
    "               xlabel='Predicted label')\n",
    "\n",
    "        # Rotate the tick labels and set their alignment.\n",
    "        plt.setp(ax_n.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "        # Loop over data dimensions (#F: i.e., cells in the confusion matrix) and create text annotations. \n",
    "        fmt = '.2f'\n",
    "        thresh = cm_norm.max() / 2.\n",
    "        for w in range(cm_norm.shape[0]):\n",
    "            for j in range(cm_norm.shape[1]):\n",
    "                ax_n.text(j, w, format(cm_norm[w, j], fmt),\n",
    "                        ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm_norm[w, j] > thresh else \"black\")\n",
    "        fig_n.tight_layout()\n",
    "        fig_n.savefig(resfile.replace('.txt','_conf_matrix_normalized.png'))\n",
    "        plt.show()\n",
    "\n",
    "########################################################################\n",
    "# Now we return the global metrics (Accuracy, Precision, Recall, F1-score) calculated above\n",
    "\n",
    "    return accuracy, global_precision, global_recall, global_f1score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8JqP7U8wVvCm",
    "outputId": "1028f917-309f-438a-cccd-238301046c98"
   },
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "###### Task 5b) Load models trained in task 4c) into two NEW models, then perform prediction for the #########\n",
    "######          test set retrieved in task 4c). Evaluate performance using performance_eval() ################\n",
    "######          Hint: use load_model for XGB and pickle.load for DNN (GIVEN IN THE CODE BELOW) ###############\n",
    "##############################################################################################################\n",
    "restestfileXGB = res_folder + '/XGB_sp_' + str(spacing) + 'w_' + str(length) + '_test_results.txt'\n",
    "restestfileDNN = res_folder + '/DNN_sp_' + str(spacing) + 'w_' + str(length) + '_test_results.txt'\n",
    "restestfileKNN = res_folder + '/KNN_sp_' + str(spacing) + 'w_' + str(length) + '_test_results.txt'\n",
    "\n",
    "# load the model from disk\n",
    "loaded_xgb = XGBClassifier()\n",
    "loaded_xgb.load_model(xgbmodelfile)\n",
    "\n",
    "# load the model from disk\n",
    "loaded_dnn = pickle.load(open(dnnmodelfile, 'rb'))\n",
    "\n",
    "# load the model from disk\n",
    "loaded_knn = pickle.load(open(knnmodelfile, 'rb'))\n",
    "\n",
    "lbl = [0, 1]\n",
    "label_names=['Attenuation', 'Filtering']\n",
    "############# ADD YOUR CODE BELOW #############\n",
    "\n",
    "# Added to fix: 'XGBClassifier' object has no attribute '_le'\n",
    "loaded_xgb._le = LabelEncoder().fit(y_test)\n",
    "\n",
    "y_pred_XGB = loaded_xgb.predict(X_test)\n",
    "y_pred_DNN = loaded_dnn.predict(X_test)\n",
    "y_pred_KNN = loaded_knn.predict(X_test)\n",
    "\n",
    "XGB_metrics = performance_eval(y_test, y_pred_XGB, lbl, label_names, restestfileXGB)\n",
    "DNN_metrics = performance_eval(y_test, y_pred_DNN, lbl, label_names, restestfileDNN)\n",
    "KNN_metrics = performance_eval(y_test, y_pred_KNN, lbl, label_names, restestfileKNN)\n",
    "\n",
    "print('XGB metrics: ' +str(XGB_metrics))\n",
    "print('****************')\n",
    "print('DNN metrics: ' +str(DNN_metrics))\n",
    "print('****************')\n",
    "print('KNN metrics: ' +str(KNN_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SeYTFGP4VvCn",
    "outputId": "c3a8c8d4-0641-49b1-de70-6dd936dff76a"
   },
   "outputs": [],
   "source": [
    "##############################################################################################################\n",
    "###### Task 6a) For window length in range(10,101,30) and window spacing = 1, repeat #########################\n",
    "######          dataset load (task 3b), training and hyperparameter optimization (task 4c), ##################\n",
    "######          and test and performance evaluation (task 5b), storing global results ########################\n",
    "######          (accuracy, precision, recall and F1-score) in proper lists for each ML algorithm #############\n",
    "######          USE ONLY THE ptp FEATURE TO DO THE ENTIRE PROCESS ############################################\n",
    "######          USE 10%/90% TRAIN/TEST SPLIT TO SPEEDUP THE PROCESS ##########################################\n",
    "######          Hints: for each metric and each ML algorithm, pre-instantiate np.arrays with dimensions ######\n",
    "######          of the space (window spacing, window length) #################################################\n",
    "##############################################################################################################\n",
    "start=time.time()\n",
    "\n",
    "\n",
    "#F: these params will be used to iterate over different values of window length...\n",
    "windowrange=list(range(10,101,30))\n",
    "#F: ...and window spacing (if needed)\n",
    "spacingrange=[1]\n",
    "\n",
    "\n",
    "lbl = [0, 1]\n",
    "label_names=['Attenuation', 'Filtering']\n",
    "\n",
    "#F: folder where to put result files\n",
    "res_folder = 'RESLOOP_Window'\n",
    "if not os.path.exists(res_folder):\n",
    "    os.makedirs(res_folder)\n",
    "\n",
    "A_XGB = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GP_XGB = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GR_XGB = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GF1_XGB = np.zeros([len(spacingrange),len(windowrange)])\n",
    "\n",
    "A_DNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GP_DNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GR_DNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GF1_DNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "\n",
    "A_KNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GP_KNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GR_KNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "GF1_KNN = np.zeros([len(spacingrange),len(windowrange)])\n",
    "############# ADD YOUR CODE BELOW #############\n",
    "\n",
    "\n",
    "for i, spacing in enumerate(spacingrange): #enumerate(range(minsp,maxsp+1,stepsp)):\n",
    "    for j, length in enumerate(windowrange): #enumerate(range(minlength,maxlength+1,steplength)):\n",
    "        print('********************************')\n",
    "        print('Iteration for spacing={} and window length={}'.format(spacing,length))\n",
    "        \n",
    "        ####### 1) Load dataset (task 3b) #######\n",
    "        print('1) Loading dataset into (XX,yy)...')\n",
    "        \n",
    "        XX = None\n",
    "        yy = None\n",
    "        folderpath='Features'\n",
    "        \n",
    "        for filename in os.listdir(folderpath):\n",
    "            if filename.endswith('_sp' + str(spacing) + '_w' + str(length) + '.dat'):\n",
    "                label = 0\n",
    "                if int(filename[9]) > 5:\n",
    "                    label = 1\n",
    "                fullname = folderpath + '/' + filename\n",
    "\n",
    "                XX, yy = load_window_dataset(XX, yy, fullname, label)\n",
    "\n",
    "        print(XX)\n",
    "        XX=XX[:,3:4] #F: use the 3rd feature only (ptp)\n",
    "        print(XX)\n",
    "        scaler = StandardScaler()\n",
    "        XX = scaler.fit_transform(XX)\n",
    "\n",
    "        ####### 2) Hyperparameters optimization and training (task 4c) #######\n",
    "        print('2) Hyperparameters optimization and training...')\n",
    "        resfile_XGB=res_folder + '/XGB_sp_' + str(spacing) + 'w_' + str(length) + '_results.txt'\n",
    "        resfile_DNN=res_folder + '/DNN_sp_' + str(spacing) + 'w_' + str(length) + '_results.txt'\n",
    "        resfile_KNN=res_folder + '/KNN_sp_' + str(spacing) + 'w_' + str(length) + '_results.txt'\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(XX, yy, stratify=yy, test_size=0.9, random_state=42)\n",
    "        print('Training XGB...')\n",
    "        xgb = train_classifier_XGB(X_train, y_train, resfile_XGB)\n",
    "\n",
    "        print('Training DNN...')\n",
    "        dnn = train_classifier_DNN(X_train, y_train, resfile_DNN)\n",
    "        \n",
    "        print('Training KNN...')\n",
    "        knn = train_classifier_KNN(X_train, y_train, resfile_KNN)\n",
    "        \n",
    "        ####### 3) Predict and performance evaluation (task 5b) #######\n",
    "        print('3) Predict and performance evaluation...')\n",
    "        restestfileXGB = res_folder + '/XGB_sp_' + str(spacing) + 'w_' + str(length) + '_test_results.txt'\n",
    "        restestfileDNN = res_folder + '/DNN_sp_' + str(spacing) + 'w_' + str(length) + '_test_results.txt'\n",
    "        restestfileKNN = res_folder + '/KNN_sp_' + str(spacing) + 'w_' + str(length) + '_test_results.txt'\n",
    "\n",
    "\n",
    "        y_pred_XGB = xgb.predict(X_test)\n",
    "        y_pred_DNN = dnn.predict(X_test)\n",
    "        y_pred_KNN = knn.predict(X_test)\n",
    "\n",
    "        A_XGB[i,j], GP_XGB[i,j], GR_XGB[i,j], GF1_XGB[i,j] = performance_eval(y_test, y_pred_XGB, lbl, label_names, restestfileXGB)\n",
    "        A_DNN[i,j], GP_DNN[i,j], GR_DNN[i,j], GF1_DNN[i,j] = performance_eval(y_test, y_pred_DNN, lbl, label_names, restestfileDNN)\n",
    "        A_KNN[i,j], GP_KNN[i,j], GR_KNN[i,j], GF1_KNN[i,j] = performance_eval(y_test, y_pred_KNN, lbl, label_names, restestfileKNN)\n",
    "\n",
    "\n",
    "end=time.time()\n",
    "\n",
    "\n",
    "print('total time=' + str(end-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "Ar7-qijGVvCn",
    "outputId": "21dcd250-9ce4-477d-f0ce-32f0d2d9cdb9"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###### Task 6b) Plot the 4 metrics vs window length in 4 separate graphs ######\n",
    "######          each graph should include one curve for each ML algorithm #####\n",
    "###############################################################################\n",
    "\n",
    "xvalues=np.array(windowrange)\n",
    "\n",
    "A_XGB = A_XGB.reshape(-1, 1)\n",
    "GP_XGB = GP_XGB.reshape(-1, 1)\n",
    "GR_XGB = GR_XGB.reshape(-1, 1)\n",
    "GF1_XGB = GF1_XGB.reshape(-1, 1)\n",
    "\n",
    "A_DNN = A_DNN.reshape(-1, 1)\n",
    "GP_DNN = GP_DNN.reshape(-1, 1)\n",
    "GR_DNN = GR_DNN.reshape(-1, 1)\n",
    "GF1_DNN = GF1_DNN.reshape(-1, 1)\n",
    "\n",
    "A_KNN = A_KNN.reshape(-1, 1)\n",
    "GP_KNN = GP_KNN.reshape(-1, 1)\n",
    "GR_KNN = GR_KNN.reshape(-1, 1)\n",
    "GF1_KNN = GF1_KNN.reshape(-1, 1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, sharex='all')\n",
    "\n",
    "axs[0,0].plot(xvalues, A_XGB, label = 'XGB')\n",
    "axs[0,0].plot(xvalues, A_DNN, label = 'DNN')\n",
    "axs[0,0].plot(xvalues, A_KNN, label = 'KNN')\n",
    "axs[0,0].set_title('Accuracy')\n",
    "axs[0,0].set_ylabel('Accuracy')\n",
    "\n",
    "axs[0,1].plot(xvalues, GP_XGB, label = 'XGB')\n",
    "axs[0,1].plot(xvalues, GP_DNN, label = 'DNN')\n",
    "axs[0,1].plot(xvalues, GP_KNN, label = 'KNN')\n",
    "axs[0,1].set_title('Precision')\n",
    "axs[0,1].set_ylabel('Precision')\n",
    "\n",
    "axs[1,0].plot(xvalues, GR_XGB, label = 'XGB')\n",
    "axs[1,0].plot(xvalues, GR_DNN, label = 'DNN')\n",
    "axs[1,0].plot(xvalues, GR_KNN, label = 'KNN')\n",
    "axs[1,0].set_title('Recall')\n",
    "axs[1,0].set_ylabel('Recall')\n",
    "\n",
    "axs[1,1].plot(xvalues, GP_XGB, label = 'XGB')\n",
    "axs[1,1].plot(xvalues, GP_DNN, label = 'DNN')\n",
    "axs[1,1].plot(xvalues, GP_KNN, label = 'KNN')\n",
    "axs[1,1].set_title('F1-Score')\n",
    "axs[1,1].set_ylabel('F1-Score')\n",
    "\n",
    "axs[1,0].set_xlabel('Window length, s')\n",
    "axs[1,1].set_xlabel('Window length, s')\n",
    "\n",
    "\n",
    "axs[1,1].legend(loc='best')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(res_folder+'/metrics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Gq7JCLErVvCo",
    "outputId": "71b35a0a-68d7-4fb2-b699-4fdabaa626b9"
   },
   "outputs": [],
   "source": [
    "!zip -r /content/files.zip /content/Features /content/Figures /content/Results /content/RESLOOP_Window\n",
    "\n",
    "from google.colab import files\n",
    "files.download(\"/content/files.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3qMz5ZC_VvCo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2i65SStVvCo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
