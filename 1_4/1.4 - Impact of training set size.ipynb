{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Patatone/Network-failure-cause-identification/blob/main/Failure_cause_identification_with_different_failures_location_IPY.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "f-Dgj8rcVvCZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marsl\\anaconda3\\envs\\ML\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hyperopt\n",
    "import time\n",
    "import sklearn.metrics as mt\n",
    "import pickle\n",
    "\n",
    "from xgboost import Booster\n",
    "from xgboost import XGBClassifier\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wtkni_HDVvCg"
   },
   "outputs": [],
   "source": [
    "#################################################################################################\n",
    "###### The function load_window_dataset() that takes in input window data file, and \n",
    "###### label to be assigned and returns numpy arrays with features and labels\n",
    "#################################################################################################\n",
    "\n",
    "def load_window_dataset(X, y, filename, label):\n",
    "#Inputs: - X: current matrix of datapoints where we want to APPEND the datapoints retrieved from filename (only features)\n",
    "#        - y: current matrix of datapoints where we want to APPEND the datapoints retrieved from filename (only labels)\n",
    "#        - filename: full name (with path) of the file to be read (it must be a window dataset file created above)\n",
    "#        - label: integer, label to be assigned to the datapoints retrieved from filename; it may differ from labels already included in current y\n",
    "#Outputs: - X: updated X (including features for the new data points retrieved from filename)\n",
    "#         - y: updated y (including labels for the new data points)\n",
    "#This function to X and y in input the new datapoints retrieved from filename and return updated X and y\n",
    "#The function handle the case when X and y are empty (initialized as None)\n",
    "\n",
    "    data = pd.read_csv(filename)\n",
    "\n",
    "    if X is None:\n",
    "        X = data.to_numpy()\n",
    "        # full() function puts in all X.shape[0] elements the value \"label\"\n",
    "        y = np.full(X.shape[0], label)\n",
    "    else:\n",
    "        X_temp = data.to_numpy()\n",
    "        y_temp = np.full(X_temp.shape[0], label)\n",
    "        X = np.append(X, X_temp, axis = 0) #F: axis=0-->stack X and X_temp vertically (increase no of rows)\n",
    "        y = np.append(y, y_temp)\n",
    "\n",
    "    return X, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dcvMwMfcVvCi",
    "outputId": "371a06b4-3fa7-42a0-efd9-c045e832947f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario_1_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (21591, 6)\n",
      "current shape of y: (21591,)\n",
      "Scenario_1_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (43182, 6)\n",
      "current shape of y: (43182,)\n",
      "Scenario_1_monitor_node_1_preamp_lpth_3-2_1_sp1_w10.dat\n",
      "current shape of X: (64773, 6)\n",
      "current shape of y: (64773,)\n",
      "Scenario_2_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (86364, 6)\n",
      "current shape of y: (86364,)\n",
      "Scenario_2_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (107955, 6)\n",
      "current shape of y: (107955,)\n",
      "Scenario_3_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (129546, 6)\n",
      "current shape of y: (129546,)\n",
      "Scenario_4_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (151137, 6)\n",
      "current shape of y: (151137,)\n",
      "Scenario_4_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (172728, 6)\n",
      "current shape of y: (172728,)\n",
      "Scenario_4_monitor_node_1_preamp_lpth_3-2_1_sp1_w10.dat\n",
      "current shape of X: (194319, 6)\n",
      "current shape of y: (194319,)\n",
      "Scenario_5_monitor_node_1_preamp_lpth_2_1_sp1_w10.dat\n",
      "current shape of X: (215910, 6)\n",
      "current shape of y: (215910,)\n",
      "Scenario_5_monitor_node_1_preamp_lpth_3-1_1_sp1_w10.dat\n",
      "current shape of X: (237500, 6)\n",
      "current shape of y: (237500,)\n",
      "Scenario_5_monitor_node_1_preamp_lpth_3-2_1_sp1_w10.dat\n",
      "current shape of X: (259090, 6)\n",
      "current shape of y: (259090,)\n",
      "Scenario_6_monitor_node_1_preamp_lpth_3-2_1_100GHz_sp1_w10.dat\n",
      "current shape of X: (259681, 6)\n",
      "current shape of y: (259681,)\n",
      "Scenario_6_monitor_node_1_preamp_lpth_3-2_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (259972, 6)\n",
      "current shape of y: (259972,)\n",
      "Scenario_6_monitor_node_1_preamp_lpth_3-2_1_25GHz_sp1_w10.dat\n",
      "current shape of X: (260263, 6)\n",
      "current shape of y: (260263,)\n",
      "Scenario_7_monitor_node_1_preamp_lpth_2_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (281854, 6)\n",
      "current shape of y: (281854,)\n",
      "Scenario_7_monitor_node_1_preamp_lpth_3-1_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (303445, 6)\n",
      "current shape of y: (303445,)\n",
      "Scenario_7_monitor_node_1_preamp_lpth_3-2_1_12.5GHz_sp1_w10.dat\n",
      "current shape of X: (325036, 6)\n",
      "current shape of y: (325036,)\n",
      "Scenario_8_monitor_node_1_preamp_lpth_2_1_50GHz_sp1_w10.dat\n",
      "current shape of X: (346627, 6)\n",
      "current shape of y: (346627,)\n",
      "Scenario_8_monitor_node_1_preamp_lpth_3-1_1_50GHz_sp1_w10.dat\n",
      "current shape of X: (368218, 6)\n",
      "current shape of y: (368218,)\n",
      "Scenario_8_monitor_node_1_preamp_lpth_3-2_1_50GHz_sp1_w10.dat\n",
      "current shape of X: (389809, 6)\n",
      "current shape of y: (389809,)\n",
      "[[18.815 18.815  0.141  0.052 18.893 18.752]\n",
      " [18.816 18.816  0.141  0.052 18.893 18.752]\n",
      " [18.824 18.824  0.141  0.048 18.893 18.752]\n",
      " ...\n",
      " [24.105 24.105  0.109  0.033 24.166 24.057]\n",
      " [24.117 24.117  0.099  0.035 24.175 24.076]\n",
      " [24.123 24.123  0.099  0.036 24.175 24.076]]\n",
      "[0 0 0 ... 1 1 1]\n",
      "(389809, 6)\n",
      "(389809,)\n",
      "[[-0.34193409 -0.34195533  0.39904486  0.81321582 -0.33558538 -0.34337995]\n",
      " [-0.34156691 -0.34158816  0.39904486  0.81321582 -0.33558538 -0.34337995]\n",
      " [-0.33862951 -0.33865076  0.39904486  0.56228842 -0.33558538 -0.34337995]\n",
      " ...\n",
      " [ 1.60042239  1.60040113 -0.25180071 -0.37868934  1.59894936  1.60628196]\n",
      " [ 1.60482849  1.60480723 -0.45518995 -0.25322564  1.60225124  1.61326472]\n",
      " [ 1.60703154  1.60701029 -0.45518995 -0.19049379  1.60225124  1.61326472]]\n",
      "[0 0 0 ... 1 1 1]\n",
      "(389809, 6)\n",
      "(389809,)\n"
     ]
    }
   ],
   "source": [
    "##########################################################################################\n",
    "###### Use function load_window_dataset() with datasets of for all scenarios  \n",
    "###### using window length = 10 and spacing = 1. Finally, perform features scaling \n",
    "##########################################################################################\n",
    "\n",
    "X=None \n",
    "y=None\n",
    "length=10\n",
    "spacing=1\n",
    "folderpath='../Features_raw'\n",
    "\n",
    "for filename in os.listdir(folderpath):   \n",
    "    if filename.endswith('_sp' + str(spacing) + '_w' + str(length) + '.dat'):\n",
    "        print(filename)\n",
    "        label = 0\n",
    "        if int(filename[9]) > 5:\n",
    "          label = 1\n",
    "        fullname = folderpath + '/' + filename\n",
    "#------------------------------------------------------------\n",
    "        X, y = load_window_dataset(X, y, fullname, label)\n",
    "#------------------------------------------------------------\n",
    "        print('current shape of X: ' +str(X.shape))\n",
    "        print('current shape of y: ' +str(y.shape))\n",
    "\n",
    "#All scenario correlated ONLY TO \"length\" and \"spacing\" variables\n",
    "print(X)\n",
    "print(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# Features scaling \n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i6dGWj1QVvCj"
   },
   "outputs": [],
   "source": [
    "best_params_XGB = None\n",
    "best_cv_acc_XGB = None\n",
    "\n",
    "def train_classifier_XGB(X_train, y_train): \n",
    "\n",
    "    global best_params_XGB\n",
    "    global best_cv_acc_XGB\n",
    "    \n",
    "    #F: define the search space for your hyperparameters - a space where to search\n",
    "    # These parameters are needed to balance between underfitting and overfitting\n",
    "    # We are testing 3 hyperparameters: eta, max_depth and subsample\n",
    "    space4xgb = { \n",
    "     'eta': hp.choice('eta', [0.1, 0.3, 0.5, 0.7, 0.9, 1]),\n",
    "     # max_depth (maximum depth of the decision trees being trained)\n",
    "     'max_depth': hp.choice('max_depth', np.arange(1, 20, 2)),\n",
    "     'subsample': hp.choice('subsample', [0.1, 0.3, 0.5, 0.7, 0.9, 1])\n",
    "    }\n",
    "\n",
    "    # hyperopt is used to perform an efficent search in the space of parameters\n",
    "    def hyperopt_train_test(params):\n",
    "        model = XGBClassifier(use_label_encoder=False, verbosity = 0, **params)\n",
    "        #F: see https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst\n",
    "        \n",
    "        return cross_val_score(model, X_train, y_train, cv = 5).mean()\n",
    "        #F: cross_val_score is from scikit learn (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)\n",
    "        #F: will use the default score (for XGB it is accuracy)\n",
    "        #F: this includes also training; cv=5 (5 number of folds) (5-folds crossvalidation)\n",
    "        #F: .mean() is taken as cross_val_score returns an array of scores (one for each fold)\n",
    "        # We have to do the mean because \"cross_val_score\" returns the accuracy of all the folds\n",
    "\n",
    "    #We pass to this function the \"space4xgb\" parameter (in fmin() function)\n",
    "    #F: this function is used below, as a parameter to fmin\n",
    "    def f(params): \n",
    "        # assumes that \"hyperopt_train_test\" gives use the best cross validation accuracy \n",
    "        # given that combination of hyperparameters (params)\n",
    "        acc = hyperopt_train_test(params)\n",
    "        #F: loss is returned as opposite (negative) of accuracy because we will use in \n",
    "        #f_min (that only minimizes), where we want to minimize the loss (i.e., maximize accuracy)\n",
    "        # We need to return these parameters because fmin() requires them\n",
    "        return {'loss': -acc, 'status': STATUS_OK} #F: loss is returned as opposite (negative) of accuracy because we will use in f_min (that only minimizes), where we want to minimize the loss (i.e., maximize accuracy)\n",
    "\n",
    "    if best_params_XGB is None:\n",
    "        trials = Trials()\n",
    "        # best_params stores the index of the best parameters values according to the function\n",
    "        # fmin() returns the indexes based on the minimum value of a passed function \"f\"\n",
    "        # space4xgb is the search space\n",
    "        # algo=tpe.suggest is the used alorithm\n",
    "        # max_evals=5 is the maximum trials\n",
    "        best_params_XGB = fmin(f, space4xgb, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "        #F: see: https://github.com/hyperopt/hyperopt/blob/master/hyperopt/fmin.py\n",
    "        #F: at this point, best_param is a dictionary where each key is the index of the corresponding best param in space4xgb\n",
    "        print(best_params_XGB)\n",
    "    \n",
    "        #Insert in the paramets the values of the hyperparameters (not the indexes)\n",
    "        best_params_XGB = hyperopt.space_eval(space4xgb, best_params_XGB)\n",
    "        #F: this is used to extract from space4xgb the best values according to the indexes in best_params (and put such values in best_params)\n",
    "        print(best_params_XGB)\n",
    "    \n",
    "        best_cv_acc_XGB = -round(trials.best_trial['result']['loss'], 2) #F: best across trials\n",
    "        print('best_cv_acc: ' + str(best_cv_acc_XGB))\n",
    "\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eta = best_params_XGB['eta'], max_depth= best_params_XGB['max_depth'], \n",
    "                            subsample = best_params_XGB['subsample'], verbosity = 0) \n",
    "\n",
    "    # Execute the traning 3 times to be sure that the training time is correct\n",
    "    ttimes = []\n",
    "    for i in range(0, 3):\n",
    "        t0 = time.time()\n",
    "        xgb.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "        ex_time = t1 - t0\n",
    "        ttimes.append(ex_time)\n",
    "        print(ex_time)\n",
    "\n",
    "    return xgb, ttimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "a0aJlN6lVvCl"
   },
   "outputs": [],
   "source": [
    "best_params_DNN = None\n",
    "best_cv_acc_DNN = None\n",
    "\n",
    "def train_classifier_DNN(X_train, y_train): \n",
    "\n",
    "    global best_params_DNN\n",
    "    global best_cv_acc_DNN\n",
    "\n",
    "    #F: define the search space for your hyperparameters\n",
    "    space4dnn = {\n",
    "     'activation': hp.choice('activation', ['logistic', 'tanh', 'relu']),\n",
    "     'neurons': hp.choice('neurons', [10, 50, 100]),\n",
    "     'layers': hp.choice('layers', np.arange(1, 4, 1))\n",
    "    }\n",
    "\n",
    "    def hyperopt_train_test(params):\n",
    "        size = (params['neurons'],) * params['layers']\n",
    "        dnn = MLPClassifier(hidden_layer_sizes=size, activation=params['activation'],\n",
    "                            solver='adam', learning_rate='invscaling', max_iter=1000)\n",
    "        return cross_val_score(dnn, X_train, y_train, cv = 5).mean()\n",
    "\n",
    "    def f(params):\n",
    "        acc = hyperopt_train_test(params)\n",
    "        return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "    if best_params_DNN is None:\n",
    "        trials = Trials()\n",
    "        best_params_DNN = fmin(f, space4dnn, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "        print(best_params_DNN) \n",
    "    \n",
    "        best_params_DNN = hyperopt.space_eval(space4dnn, best_params_DNN)\n",
    "        print(best_params_DNN) \n",
    "    \n",
    "        best_cv_acc_DNN = -round(trials.best_trial['result']['loss'], 2)\n",
    "        print('best_cv_acc: ' + str(best_cv_acc_DNN))\n",
    "    \n",
    "    \n",
    "    size = (best_params_DNN['neurons'],) * best_params_DNN['layers']\n",
    "    dnn = MLPClassifier(hidden_layer_sizes=size, activation=best_params_DNN['activation'],\n",
    "                                solver='adam', learning_rate='invscaling', max_iter=1000)\n",
    "\n",
    "    # Execute the traning 3 times to be sure that the training time is correct\n",
    "    ttimes = []\n",
    "    for i in range(0, 3):\n",
    "        t0 = time.time()\n",
    "        dnn.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "        ex_time = t1 - t0\n",
    "        ttimes.append(ex_time)\n",
    "\n",
    "    return dnn, ttimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SHnCDjAsPrmp"
   },
   "outputs": [],
   "source": [
    "best_params_KNN = None\n",
    "best_cv_acc_KNN = None\n",
    "\n",
    "def train_classifier_KNN(X_train, y_train): \n",
    "\n",
    "    global best_params_KNN\n",
    "    global best_cv_acc_KNN\n",
    "\n",
    "    #F: define the search space for your hyperparameters\n",
    "    space4knn = {\n",
    "     'leaf_size': hp.choice('leaf_size', np.arange(1, 50, 1)),\n",
    "     'p': hp.choice('p', [1, 2]),\n",
    "     'n_neighbors': hp.choice('n_neighbors', np.arange(1, 30, 1))\n",
    "    }\n",
    "\n",
    "    def hyperopt_train_test(params):\n",
    "        knn = KNeighborsClassifier(leaf_size=params['leaf_size'], p=params['p'], \n",
    "                                   n_neighbors=params['n_neighbors'])\n",
    "        return cross_val_score(knn, X_train, y_train, cv = 5).mean()\n",
    "\n",
    "    def f(params):\n",
    "        acc = hyperopt_train_test(params)\n",
    "        return {'loss': -acc, 'status': STATUS_OK}\n",
    "\n",
    "    if best_params_KNN is None:\n",
    "        trials = Trials()\n",
    "        best_params_KNN = fmin(f, space4knn, algo=tpe.suggest, max_evals=5, trials=trials)\n",
    "        print(best_params_KNN) \n",
    "    \n",
    "        best_params_KNN = hyperopt.space_eval(space4knn, best_params_KNN)\n",
    "        print(best_params_KNN) \n",
    "    \n",
    "        best_cv_acc_KNN = -round(trials.best_trial['result']['loss'], 2)\n",
    "        print('best_cv_acc: ' + str(best_cv_acc_KNN))\n",
    "    \n",
    "    knn = KNeighborsClassifier(leaf_size=best_params_KNN['leaf_size'], p=best_params_KNN['p'], \n",
    "                                   n_neighbors=best_params_KNN['n_neighbors'])\n",
    "\n",
    "    # Execute the traning 3 times to be sure that the training time is correct\n",
    "    ttimes = []\n",
    "    for i in range(0, 3):\n",
    "        t0 = time.time()\n",
    "        knn.fit(X_train, y_train)\n",
    "        t1 = time.time()\n",
    "        ex_time = t1 - t0\n",
    "        ttimes.append(ex_time)\n",
    "        print(ex_time)\n",
    "\n",
    "    return knn, ttimes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BF7AXS_bVvCl",
    "outputId": "f6fff8fd-3a66-4554-a277-782e6000d604",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training XGB with training size: 0.03%...\n",
      "100%|██████████| 5/5 [00:01<00:00,  2.80trial/s, best loss: -0.9742753623188406]\n",
      "{'eta': 1, 'max_depth': 6, 'subsample': 2}\n",
      "{'eta': 0.3, 'max_depth': 13, 'subsample': 0.5}\n",
      "best_cv_acc: 0.97\n",
      "0.0752096176147461\n",
      "0.07977437973022461\n",
      "0.07075119018554688\n",
      "Training DNN with training size: 0.03%...\n",
      "100%|██████████| 5/5 [00:09<00:00,  1.94s/trial, best loss: -0.9829710144927537]\n",
      "{'activation': 2, 'layers': 1, 'neurons': 0}\n",
      "{'activation': 'relu', 'layers': 2, 'neurons': 10}\n",
      "best_cv_acc: 0.98\n",
      "Training KNN with training size: 0.03%...\n",
      "100%|██████████| 5/5 [00:00<00:00, 27.29trial/s, best loss: -0.9568840579710145]\n",
      "{'leaf_size': 40, 'n_neighbors': 16, 'p': 0}\n",
      "{'leaf_size': 41, 'n_neighbors': 17, 'p': 1}\n",
      "best_cv_acc: 0.96\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "Training XGB with training size: 0.05%...\n",
      "0.07561564445495605\n",
      "0.06971096992492676\n",
      "0.07639050483703613\n",
      "Training DNN with training size: 0.05%...\n",
      "Training KNN with training size: 0.05%...\n",
      "0.0026831626892089844\n",
      "0.0\n",
      "0.0\n",
      "Training XGB with training size: 0.1%...\n",
      "0.06974554061889648\n",
      "0.07885336875915527\n",
      "0.07787036895751953\n",
      "Training DNN with training size: 0.1%...\n",
      "Training KNN with training size: 0.1%...\n",
      "0.0045125484466552734\n",
      "0.003136873245239258\n",
      "0.0005071163177490234\n",
      "Training XGB with training size: 0.5%...\n",
      "0.1237649917602539\n",
      "0.14803838729858398\n",
      "0.13840103149414062\n",
      "Training DNN with training size: 0.5%...\n",
      "Training KNN with training size: 0.5%...\n",
      "0.0039958953857421875\n",
      "0.004897356033325195\n",
      "0.0033643245697021484\n",
      "Training XGB with training size: 1.0%...\n",
      "0.19798970222473145\n",
      "0.1898488998413086\n",
      "0.17729830741882324\n",
      "Training DNN with training size: 1.0%...\n",
      "Training KNN with training size: 1.0%...\n",
      "0.0\n",
      "0.0156252384185791\n",
      "0.0\n",
      "Training XGB with training size: 2.0%...\n",
      "0.33782196044921875\n",
      "0.37082886695861816\n",
      "0.34852147102355957\n",
      "Training DNN with training size: 2.0%...\n",
      "Training KNN with training size: 2.0%...\n",
      "0.01679706573486328\n",
      "0.012545108795166016\n",
      "0.01399850845336914\n",
      "Training XGB with training size: 4.0%...\n",
      "0.594592809677124\n",
      "0.6923923492431641\n",
      "0.6419060230255127\n",
      "Training DNN with training size: 4.0%...\n",
      "Training KNN with training size: 4.0%...\n",
      "0.018996715545654297\n",
      "0.019001483917236328\n",
      "0.025998353958129883\n",
      "Training XGB with training size: 6.0%...\n",
      "0.8769106864929199\n",
      "0.8561303615570068\n",
      "0.887401819229126\n",
      "Training DNN with training size: 6.0%...\n",
      "Training KNN with training size: 6.0%...\n",
      "0.04564666748046875\n",
      "0.04082942008972168\n",
      "0.03800678253173828\n",
      "Training XGB with training size: 8.0%...\n",
      "1.2218284606933594\n",
      "1.2008912563323975\n",
      "1.2097108364105225\n",
      "Training DNN with training size: 8.0%...\n",
      "Training KNN with training size: 8.0%...\n",
      "0.06484651565551758\n",
      "0.05417966842651367\n",
      "0.06412816047668457\n",
      "Training XGB with training size: 10.0%...\n",
      "1.5887701511383057\n",
      "1.6124098300933838\n",
      "1.5621893405914307\n",
      "Training DNN with training size: 10.0%...\n",
      "Training KNN with training size: 10.0%...\n",
      "0.06439375877380371\n",
      "0.06322574615478516\n",
      "0.06411457061767578\n",
      "Training XGB with training size: 20.0%...\n",
      "3.764547348022461\n",
      "3.762880802154541\n",
      "3.8779096603393555\n",
      "Training DNN with training size: 20.0%...\n",
      "Training KNN with training size: 20.0%...\n",
      "0.16878604888916016\n",
      "0.15788531303405762\n",
      "0.1715562343597412\n",
      "Training XGB with training size: 30.0%...\n",
      "6.310159206390381\n",
      "6.40110969543457\n",
      "6.1491968631744385\n",
      "Training DNN with training size: 30.0%...\n",
      "Training KNN with training size: 30.0%...\n",
      "0.2635312080383301\n",
      "0.3038337230682373\n",
      "0.24378728866577148\n",
      "Training XGB with training size: 40.0%...\n",
      "9.122061729431152\n",
      "9.115279912948608\n",
      "9.204289197921753\n",
      "Training DNN with training size: 40.0%...\n",
      "Training KNN with training size: 40.0%...\n",
      "0.7085323333740234\n",
      "0.7407829761505127\n",
      "0.4951663017272949\n",
      "Training XGB with training size: 50.0%...\n",
      "11.797431468963623\n",
      "14.961606740951538\n",
      "15.0177583694458\n",
      "Training DNN with training size: 50.0%...\n",
      "Training KNN with training size: 50.0%...\n",
      "0.5249662399291992\n",
      "0.6089017391204834\n",
      "0.5259828567504883\n",
      "Training XGB with training size: 70.0%...\n",
      "23.718291997909546\n",
      "18.447925329208374\n",
      "17.828933238983154\n",
      "Training DNN with training size: 70.0%...\n",
      "Training KNN with training size: 70.0%...\n",
      "0.9269003868103027\n",
      "0.9132287502288818\n",
      "0.9197404384613037\n",
      "Training XGB with training size: 90.0%...\n",
      "25.906578302383423\n",
      "25.341922998428345\n",
      "26.181244611740112\n",
      "Training DNN with training size: 90.0%...\n",
      "Training KNN with training size: 90.0%...\n",
      "1.159759521484375\n",
      "1.2445242404937744\n",
      "1.204590082168579\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################\n",
    "###### Split into into train/test and call train_classifier_XXX() functions\n",
    "################################################################################################################\n",
    "\n",
    "# Stratify garantees the split all the scenarios among train and test\n",
    "# It's like shuffle and split\n",
    "# random_state it's a seed to get the same output\n",
    "\n",
    "xgb_all_ttimes = []\n",
    "dnn_all_ttimes = []\n",
    "knn_all_ttimes = []\n",
    "\n",
    "xgb_all_models = []\n",
    "dnn_all_models = []\n",
    "knn_all_models = []\n",
    "\n",
    "all_X_train_X_test_y_train_y_test = []\n",
    "\n",
    "percentages = [0.0003, 0.0005, 0.001, 0.005, 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9]\n",
    "\n",
    "for i in percentages:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, train_size=i, random_state=42)\n",
    "    all_X_train_X_test_y_train_y_test.append([X_train, X_test, y_train, y_test])\n",
    "    \n",
    "    print(f'Training XGB with training size: {i*100}%...')\n",
    "    xgb, xgb_ttimes = train_classifier_XGB(X_train, y_train)\n",
    "    print(f'Training DNN with training size: {i*100}%...')\n",
    "    dnn, dnn_ttimes = train_classifier_DNN(X_train, y_train)\n",
    "    print(f'Training KNN with training size: {i*100}%...')\n",
    "    knn, knn_ttimes = train_classifier_KNN(X_train, y_train)\n",
    "    \n",
    "    xgb_all_ttimes.append(xgb_ttimes)\n",
    "    dnn_all_ttimes.append(dnn_ttimes)\n",
    "    knn_all_ttimes.append(knn_ttimes)\n",
    "    \n",
    "    xgb_all_models.append(xgb)\n",
    "    dnn_all_models.append(dnn)\n",
    "    knn_all_models.append(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "dosJAeJhVvCm"
   },
   "outputs": [],
   "source": [
    "########################################################################################################\n",
    "###### The function performance_eval() takes in input ground truth and predicted labels, \n",
    "###### prints results in a result file passed in input, and returns global metrics\n",
    "########################################################################################################\n",
    "\n",
    "def performance_eval(y_true, y_pred, lab, l_names):\n",
    "\n",
    "    #Compute metrics and print/write them\n",
    "    accuracy = mt.accuracy_score(y_true, y_pred)\n",
    "    precision = mt.precision_score(y_true, y_pred, labels=lab, average=None) #F: average=None gives per-class results\n",
    "    global_precision = mt.precision_score(y_true, y_pred, labels=lab, average='weighted') \n",
    "    recall = mt.recall_score(y_true, y_pred, labels=lab, average=None)\n",
    "    global_recall = mt.recall_score(y_true, y_pred, labels=lab, average='weighted') \n",
    "    f1score = mt.f1_score(y_true, y_pred, labels=lab, average=None)\n",
    "    global_f1score = mt.f1_score(y_true, y_pred, labels=lab, average='weighted')\n",
    "\n",
    "    return accuracy, global_precision, global_recall, global_f1score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "8JqP7U8wVvCm",
    "outputId": "1028f917-309f-438a-cccd-238301046c98"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB metrics for training size 0.03%: (0.9924658641546038, 0.9925432279977482, 0.9924658641546038, 0.9924450505481046)\n",
      "****************\n",
      "MLP metrics for training size 0.03%: (0.9757629723910873, 0.9758466484669789, 0.9757629723910873, 0.9756537032760337)\n",
      "****************\n",
      "KNN metrics for training size 0.03%: (0.9627578632410642, 0.9646581782578361, 0.9627578632410642, 0.962180331083745)\n",
      "XGB metrics for training size 0.05%: (0.9925002887465831, 0.9925732366867256, 0.9925002887465831, 0.9924800760845869)\n",
      "****************\n",
      "MLP metrics for training size 0.05%: (0.9835221949873593, 0.9836740635634029, 0.9835221949873593, 0.9834487258025248)\n",
      "****************\n",
      "KNN metrics for training size 0.05%: (0.9679760789497325, 0.9692783047792399, 0.9679760789497325, 0.9675714710825332)\n",
      "XGB metrics for training size 0.1%: (0.9839068358070977, 0.9839881541743128, 0.9839068358070977, 0.9839301568450659)\n",
      "****************\n",
      "MLP metrics for training size 0.1%: (0.9863283858045299, 0.9864809765034546, 0.9863283858045299, 0.9862709193026424)\n",
      "****************\n",
      "KNN metrics for training size 0.1%: (0.9809357506034616, 0.9812962876690076, 0.9809357506034616, 0.9808127136284988)\n",
      "XGB metrics for training size 0.5%: (0.995026555973805, 0.9950575103319926, 0.995026555973805, 0.9950178851233852)\n",
      "****************\n",
      "MLP metrics for training size 0.5%: (0.9948486567318104, 0.9948592329704374, 0.9948486567318104, 0.9948424997497894)\n",
      "****************\n",
      "KNN metrics for training size 0.5%: (0.9887278915072448, 0.9888131530911162, 0.9887278915072448, 0.9886918892472404)\n",
      "XGB metrics for training size 1.0%: (0.9979373482486895, 0.9979373356693028, 0.9979373482486895, 0.9979368336316156)\n",
      "****************\n",
      "MLP metrics for training size 1.0%: (0.9948796484163447, 0.9948859484953252, 0.9948796484163447, 0.9948744193898419)\n",
      "****************\n",
      "KNN metrics for training size 1.0%: (0.9924464449056907, 0.9924941111571682, 0.9924464449056907, 0.9924291598837802)\n",
      "XGB metrics for training size 2.0%: (0.9983717831592118, 0.998371717570938, 0.9983717831592118, 0.9983714961122171)\n",
      "****************\n",
      "MLP metrics for training size 2.0%: (0.9956284210223212, 0.995642147974021, 0.9956284210223212, 0.9956230111882917)\n",
      "****************\n",
      "KNN metrics for training size 2.0%: (0.9936494307785337, 0.9936887534180384, 0.9936494307785337, 0.9936365352328846)\n",
      "XGB metrics for training size 4.0%: (0.998530264525663, 0.9985302182374778, 0.998530264525663, 0.9985300262112081)\n",
      "****************\n",
      "MLP metrics for training size 4.0%: (0.9973037034661707, 0.9973065783656135, 0.9973037034661707, 0.997302034949503)\n",
      "****************\n",
      "KNN metrics for training size 4.0%: (0.99465283511973, 0.9946779396062848, 0.99465283511973, 0.9946440843441863)\n",
      "XGB metrics for training size 6.0%: (0.9985017234274236, 0.998501643112992, 0.9985017234274236, 0.9985014980376596)\n",
      "****************\n",
      "MLP metrics for training size 6.0%: (0.9970907780940503, 0.9970949993116472, 0.9970907780940503, 0.9970886806208112)\n",
      "****************\n",
      "KNN metrics for training size 6.0%: (0.995011202960529, 0.9950314405727602, 0.995011202960529, 0.9950038118176737)\n",
      "XGB metrics for training size 8.0%: (0.9985249215754618, 0.9985248831246745, 0.9985249215754618, 0.9985246765545753)\n",
      "****************\n",
      "MLP metrics for training size 8.0%: (0.9980453119553852, 0.9980472735502696, 0.9980453119553852, 0.9980443589932022)\n",
      "****************\n",
      "KNN metrics for training size 8.0%: (0.9955022655977692, 0.995517197110379, 0.9955022655977692, 0.995496479256122)\n",
      "XGB metrics for training size 10.0%: (0.9985691034663611, 0.9985690704385194, 0.9985691034663611, 0.9985688710717245)\n",
      "****************\n",
      "MLP metrics for training size 10.0%: (0.9979933243831041, 0.9979954302796992, 0.9979933243831041, 0.9979923135397698)\n",
      "****************\n",
      "KNN metrics for training size 10.0%: (0.9957956725356227, 0.9958081405043246, 0.9957956725356227, 0.9957907046123622)\n",
      "XGB metrics for training size 20.0%: (0.9986403632538929, 0.9986402770513252, 0.9986403632538929, 0.9986401955534284)\n",
      "****************\n",
      "MLP metrics for training size 20.0%: (0.9981882199020036, 0.9981881959906919, 0.9981882199020036, 0.9981878306179454)\n",
      "****************\n",
      "KNN metrics for training size 20.0%: (0.9964982940406865, 0.9965054598314963, 0.9964982940406865, 0.9964950788866713)\n",
      "XGB metrics for training size 30.0%: (0.9988565858092038, 0.9988566642065321, 0.9988565858092038, 0.9988563935518803)\n",
      "****************\n",
      "MLP metrics for training size 30.0%: (0.9981712702525406, 0.9981715470497755, 0.9981712702525406, 0.9981707513764403)\n",
      "****************\n",
      "KNN metrics for training size 30.0%: (0.9967163489905339, 0.9967228243600204, 0.9967163489905339, 0.9967134970463248)\n"
     ]
    }
   ],
   "source": [
    "lbl = [0, 1]\n",
    "label_names=['Attenuation', 'Filtering']\n",
    "\n",
    "XGB_all_metrics = []\n",
    "DNN_all_metrics = []\n",
    "KNN_all_metrics = []\n",
    "    \n",
    "for i, val in enumerate(percentages):\n",
    "    X_train = all_X_train_X_test_y_train_y_test[i][0]\n",
    "    X_test = all_X_train_X_test_y_train_y_test[i][1]\n",
    "    y_train = all_X_train_X_test_y_train_y_test[i][2]\n",
    "    y_test = all_X_train_X_test_y_train_y_test[i][3]\n",
    "    \n",
    "    # Added to fix: 'XGBClassifier' object has no attribute '_le'\n",
    "    xgb._le = LabelEncoder().fit(y_test)\n",
    "\n",
    "    y_pred_XGB = xgb_all_models[i].predict(X_test)\n",
    "    y_pred_DNN = dnn_all_models[i].predict(X_test)\n",
    "    y_pred_KNN = knn_all_models[i].predict(X_test)\n",
    "\n",
    "    XGB_simgle_metrics = performance_eval(y_test, y_pred_XGB, lbl, label_names)\n",
    "    DNN_single_metrics = performance_eval(y_test, y_pred_DNN, lbl, label_names)\n",
    "    KNN_single_metrics = performance_eval(y_test, y_pred_KNN, lbl, label_names)\n",
    "\n",
    "    XGB_all_metrics.append(XGB_simgle_metrics)\n",
    "    DNN_all_metrics.append(DNN_single_metrics)\n",
    "    KNN_all_metrics.append(KNN_single_metrics)\n",
    "\n",
    "    print('XGB metrics for training size '+str(val*100)+ '%: '+str(XGB_simgle_metrics))\n",
    "    print('****************')\n",
    "    print('MLP metrics for training size '+str(val*100)+ '%: '+str(DNN_single_metrics))\n",
    "    print('****************')\n",
    "    print('KNN metrics for training size '+str(val*100)+ '%: '+str(KNN_single_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages_lables = [str(x*100)+'%' for x in percentages]\n",
    "    \n",
    "def candle_trainng_size_impact(fig_folder, alg_name, ttimes):\n",
    "    global percentages\n",
    "    global percentages_lables\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    bplots = []\n",
    "    n_perc = len(percentages)\n",
    "    \n",
    "    for i in range(n_perc):\n",
    "        bplots.append(ax.boxplot(ttimes[i], positions = [i], patch_artist=True))\n",
    "\n",
    "    for bplot in bplots:\n",
    "        for patch in bplot['boxes']:\n",
    "            patch.set_facecolor('lightblue')\n",
    "    \n",
    "    plt.xticks(color='black')\n",
    "    plt.yticks(color='black')\n",
    "    plt.grid(1)\n",
    "    plt.xticks(ticks = list(range(n_perc)), labels = percentages_lables, fontsize = 14)\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=-45, ha=\"left\", rotation_mode=\"anchor\") \n",
    "    plt.ylabel('Seconds', color='black', fontsize=14)\n",
    "    image_title=alg_name+' Training set size impact on Traning Time'\n",
    "    plt.title(image_title, fontsize=14)\n",
    "    plt.show()\n",
    "    fig.savefig(fig_folder+'/'+image_title.replace(\" \", \"_\")+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_labels = ['Accuracy', 'Global Precision', 'Global Recall', 'Global F1-score']\n",
    "\n",
    "fig_folder = '1_4_Figures'\n",
    "if not os.path.exists(fig_folder):\n",
    "    os.makedirs(fig_folder)\n",
    "\n",
    "candle_trainng_size_impact(fig_folder, 'XGB', xgb_all_ttimes)\n",
    "candle_trainng_size_impact(fig_folder, 'MLP', dnn_all_ttimes)\n",
    "candle_trainng_size_impact(fig_folder, 'KNN', knn_all_ttimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###### Plot the 4 metrics vs training set size in 4 separate graphs\n",
    "###############################################################################\n",
    "        \n",
    "xvalues=np.array(percentages_lables)\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20,15))\n",
    "\n",
    "A_XGB = [item[0] for item in XGB_all_metrics]\n",
    "A_DNN = [item[0] for item in DNN_all_metrics]\n",
    "A_KNN = [item[0] for item in KNN_all_metrics]\n",
    "axs[0,0].plot(xvalues, A_XGB, label = 'XGB')\n",
    "axs[0,0].plot(xvalues, A_DNN, label = 'MLP')\n",
    "axs[0,0].plot(xvalues, A_KNN, label = 'KNN')\n",
    "axs[0,0].tick_params(labelrotation=55)\n",
    "axs[0,0].set_title('Accuracy')\n",
    "axs[0,0].set_ylabel('Accuracy')\n",
    "axs[0,0].yaxis.grid(True)\n",
    "\n",
    "GP_XGB = [item[1] for item in XGB_all_metrics]\n",
    "GP_DNN = [item[1] for item in DNN_all_metrics]\n",
    "GP_KNN = [item[1] for item in KNN_all_metrics]\n",
    "axs[0,1].plot(xvalues, GP_XGB, label = 'XGB')\n",
    "axs[0,1].plot(xvalues, GP_DNN, label = 'MLP')\n",
    "axs[0,1].plot(xvalues, GP_KNN, label = 'KNN')\n",
    "axs[0,1].tick_params(labelrotation=55)\n",
    "axs[0,1].set_title('Precision')\n",
    "axs[0,1].set_ylabel('Precision')\n",
    "axs[0,1].yaxis.grid(True)\n",
    "\n",
    "GR_XGB = [item[2] for item in XGB_all_metrics]\n",
    "GR_DNN = [item[2] for item in DNN_all_metrics]\n",
    "GR_KNN = [item[2] for item in KNN_all_metrics]\n",
    "axs[1,0].plot(xvalues, GR_XGB, label = 'XGB')\n",
    "axs[1,0].plot(xvalues, GR_DNN, label = 'MLP')\n",
    "axs[1,0].plot(xvalues, GR_KNN, label = 'KNN')\n",
    "axs[1,0].tick_params(labelrotation=55)\n",
    "axs[1,0].set_title('Recall')\n",
    "axs[1,0].set_ylabel('Recall')\n",
    "axs[1,0].yaxis.grid(True)\n",
    "\n",
    "GF1_XGB = [item[3] for item in XGB_all_metrics]\n",
    "GF1_DNN = [item[3] for item in DNN_all_metrics]\n",
    "GF1_KNN = [item[3] for item in KNN_all_metrics]\n",
    "axs[1,1].plot(xvalues, GF1_XGB, label = 'XGB')\n",
    "axs[1,1].plot(xvalues, GF1_DNN, label = 'MLP')\n",
    "axs[1,1].plot(xvalues, GF1_KNN, label = 'KNN')\n",
    "axs[1,1].tick_params(labelrotation=55)\n",
    "axs[1,1].set_title('F1-Score')\n",
    "axs[1,1].set_ylabel('F1-Score')\n",
    "axs[1,1].yaxis.grid(True)\n",
    "\n",
    "axs[1,0].set_xlabel('Train size, percentage')\n",
    "axs[1,1].set_xlabel('Train size, percentage')\n",
    "\n",
    "\n",
    "axs[1,1].legend(loc='best')\n",
    "fig.suptitle('Impact of training set size on metrics')\n",
    "    \n",
    "fig.tight_layout()\n",
    "fig.savefig(fig_folder+'/metrics.png')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
